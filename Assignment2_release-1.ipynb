{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO3406 - Introduction to Data Analytics\n",
    "## Assignment 2 -  Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Instructions **\n",
    "\n",
    "#### Kaggle Display Advertising Challenge dataset will be used in this assignment. It contains of 39 features of online ads and information of if each ad was clicked or not over a period of 7 days. The semantic of these features is undisclosed. The overall objective is to determine if an ad will be clicked or not, for a set of query of features.\n",
    "\n",
    "#### For this assignment, only 100,000 ads will be used. The dataset should be downloaded according to the instructions in part 1). The first column of *\"dac_sample.txt\"*  indicates if an ad was clicked (=1) or not (=0) while rest of the 39 columns contain feature values. For this assignment you can consider all features are categorical. The values of some of these features have been hashed for anonymization purposes. Note that some features have missing values.\n",
    "\n",
    "\n",
    "\n",
    "You may view spark stages from\n",
    "http://localhost:4040/stages/ \n",
    "\n",
    "\n",
    "Some other useful resources/references:\n",
    "+ [Map-Reduce for Machine Learning on Multicore](http://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf)\n",
    "+ [Spark RDD](http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf)\n",
    "+ [Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)\n",
    "+ [MLlib](https://spark.apache.org/docs/1.1.0/mllib-guide.html)\n",
    "+ [MLlib: Scalable Machine Learning on Spark](http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf)\n",
    "+ [Scalable Machine Learning](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Preparing the dataset **\n",
    "\n",
    "#### Import *\"assignment2LoadData.py\"* file to the [*jupyter*  home](http://localhost:8001/tree) folder.  Go to [http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/) to download the dataset. After you accept the agreement, you can obtain the dataset by clicking on the ***\"Download Sample\"*** button.  The file is 8.4 MB compressed (.tar.gz). Import the compressed data file to the  [*jupyter*  home](http://localhost:8001/tree) folder. The script in *\"assignment2LoadData.py\"* will automatically extract the file to the virtual machine (VM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName is data/Assignment2/dac_sample.txt\n",
      "File is already available. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "import assignment2LoadData as ld\n",
    "import os\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pyspark\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from random import randint\n",
    "\n",
    "url = os.getcwd()\n",
    "ld.extractData(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of rawData entry:  [u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n",
      "\n",
      "rawData count= 100000\n"
     ]
    }
   ],
   "source": [
    "#Extract\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('Assignment2', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "partitions = 1\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, partitions)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "    print 'An example of rawData entry: ', rawData.take(1)\n",
    "    nData = rawData.count()\n",
    "    print '\\nrawData count=', nData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawTrainData count= 90027\n",
      "rawTestData count= 9973\n"
     ]
    }
   ],
   "source": [
    "weights = [.9, .1]\n",
    "seed = 17\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nTest = rawTestData.count()\n",
    "print 'rawTrainData count=', nTrain\n",
    "print 'rawTestData count=', nTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Solutions: **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Creates a list of (featureID, value) tuples for each ad\n",
    "def parsePoint(point):\n",
    "    items = point.split(',')\n",
    "    clean_items = items.fetch(14)\n",
    "    return [(i, item) for i, item in enumerate(items[1:])]\n",
    "\n",
    "parsedTrainFeat = rawTrainData.map(parsePoint)\n",
    "\n",
    "# (featureID,value)\n",
    "numCategories = (parsedTrainFeat\n",
    "                 .flatMap(lambda x: x)\n",
    "                 .distinct().map(lambda x: (x[0], 1))\n",
    "                 .reduceByKey(lambda x,y: x+y)\n",
    "                 .sortByKey()\n",
    "                 .collect())\n",
    "\n",
    "# Create a One-hot dictionary from parsedTrainFeat\n",
    "def createOneHotDict(inputData):\n",
    "    distinctFeats = inputData.flatMap(lambda x: x).distinct()\n",
    "    return distinctFeats.zipWithIndex().collectAsMap()\n",
    "\n",
    "ctrOHEDict = createOneHotDict(parsedTrainFeat)\n",
    "\n",
    "# Counts the number of features\n",
    "numCtrOHEFeats = len(ctrOHEDict.keys())\n",
    "\n",
    "# Create a SparseVector from a list of features and OHE dictionary\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    sizeList = [OHEDict[f] for f in rawFeats]\n",
    "    sortedSizeList = sorted(sizeList)\n",
    "    valueList = [1 for f in rawFeats]\n",
    "    return SparseVector(numOHEFeats, sortedSizeList, valueList)\n",
    "\n",
    "# Obtain the label and feature vector for this raw observation\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    parsedPoints = parsePoint(point)\n",
    "    items = point.split(',')\n",
    "    label = items[0]\n",
    "    features = oneHotEncoding(parsedPoints, OHEDict, numOHEFeats)\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "# # Performs OHE on raw training data\n",
    "OHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "\n",
    "# # Cache the training data\n",
    "OHETrainData.cache()\n",
    "\n",
    "# # Checking to see if we get ma\n",
    "print(OHETrainData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "# Create buckets where our buckets are defined by random Centroids\n",
    "def centroid(indexList):\n",
    "    valueList = np.ones(39)\n",
    "    return SparseVector(254310, np.sort(indexList), valueList)\n",
    "\n",
    "# Parses the Centrod point and returns a new Label and Feature\n",
    "def parseCentroidPoint(indexList):\n",
    "    label = 2\n",
    "    features = centroid(indexList)\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "# Choose random centroids and assign them to our indexList\n",
    "def createRandomCentroids(num_centroids):\n",
    "    indexList = np.zeros((num_centroids,39))\n",
    "    for i in range(0,num_centroids):\n",
    "        for j in range(0,39):\n",
    "            indexList[i,j] = randint(0,254310)\n",
    "    \n",
    "    centroid = map(lambda point: parseCentroidPoint(point), indexList)\n",
    "    return centroid\n",
    "\n",
    "k = 20\n",
    "\n",
    "n_train_data = nTrain\n",
    "\n",
    "centroids = createRandomCentroids(k)\n",
    "\n",
    "train_data = OHETrainData.take(n_train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Hamming Distance \n",
    "def hammingDistance(list_one, list_two):\n",
    "    index_1, index_2 = 0, 0\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if index_1 >= list_one.size or index_2 >= list_two.size:\n",
    "            break\n",
    "        elif list_one[index_1] == list_two[index_2]:\n",
    "            counter = counter + 1\n",
    "            if index_1 < list_one.size:\n",
    "                index_1 = index_1 + 1\n",
    "            else:\n",
    "                index_2 = index_2 + 1\n",
    "        elif list_one[index_1] > list_two[index_2]:\n",
    "            if index_2 < list_two.size:\n",
    "                index_2 = index_2 + 1\n",
    "            else: \n",
    "                break\n",
    "        else:\n",
    "            if index_1 < list_one.size:\n",
    "                index_1 = index_1 + 1\n",
    "            else: \n",
    "                break\n",
    "    return counter\n",
    "\n",
    "# Calculates the cosine length\n",
    "def cosineLength(list_one, list_two):\n",
    "    list_one_big = list_one.astype(np.int64)\n",
    "    list_two_big = list_two.astype(np.int64)\n",
    "    if list_one.size < list_two.size:\n",
    "        c = list_two_big.copy()\n",
    "        c[:len(list_one_big)] += list_one_big\n",
    "        list_one_big = c\n",
    "    return ((list_one_big.dot(list_two_big))/(np.sqrt(((list_one_big) ** 2).sum()) * np.sqrt(((list_two_big) ** 2).sum())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate buckets for the training data\n",
    "dist = np.zeros((n_train_data,k))\n",
    "for index in range(0,n_train_data):\n",
    "    dist[index,:] = map(lambda point: cosineLength(train_data[index].features.indices,point.features.indices), centroids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine the max distance from cosine, this indicates that there better similarity between Sparse vectors\n",
    "max_dist = dist.argmax(axis=1)\n",
    "\n",
    "# Setup Bucket for our training data \n",
    "bucket_train = np.zeros((k,n_train_data))\n",
    "bucket_train.fill(-1)\n",
    "\n",
    "# print bucket_train\n",
    "for index, point in enumerate(max_dist):\n",
    "    bucket_train[point,index] = index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,(254310,[9418,23256,26962,34497,41035,47120,51298,51384,53140,54271,68347,73558,74217,88528,89594,100231,103630,121085,130235,131673,140921,142403,149508,151452,155024,155309,165333,189323,192945,198438,204764,215007,224556,234176,241192,241991,248964,251902],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n"
     ]
    }
   ],
   "source": [
    "# Parse Test Data\n",
    "def testOneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    validFeatureTuples = []\n",
    "    for (featID, value) in rawFeats:\n",
    "        try:\n",
    "            validFeatureTuples.append((OHEDict[(featID, value)],1))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return SparseVector(numOHEFeats, validFeatureTuples)\n",
    "\n",
    "# Parse OHE points\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    parsedPoints = parsePoint(point)\n",
    "    items = point.split(',')\n",
    "    label = items[0]\n",
    "    features = testOneHotEncoding(parsedPoints, OHEDict, numOHEFeats)\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "# # Performs OHE on raw training data\n",
    "OHETestData = rawTestData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "\n",
    "# # Cache the training data\n",
    "OHETestData.cache()\n",
    "\n",
    "# Test data\n",
    "n_test_data = nTest \n",
    "\n",
    "\n",
    "test_data = OHETestData.take(n_test_data)\n",
    "\n",
    "# # Checking to see if we get the right data\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate buckets\n",
    "dist = np.zeros((n_test_data,k))\n",
    "for index in range(0,n_test_data):\n",
    "    dist[index,:] = map(lambda point: cosineLength(test_data[index].features.indices,point.features.indices), centroids)\n",
    "    \n",
    "test_bucket = dist.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 1)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 1)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 0)\n",
      "('Prediction: ', 0, 'Actual: ', 1)\n",
      "('Prediction: ', 0, 'Actual: ', 0)"
     ]
    }
   ],
   "source": [
    "# Calculate the Euclidean distance for KNN\n",
    "def euclideanLength(list_one, list_two):\n",
    "    list_one_big = list_one.astype(np.int64)\n",
    "    list_two_big = list_two.astype(np.int64)\n",
    "    if list_one.size < list_two.size:\n",
    "        c = list_two_big.copy()\n",
    "        c[:len(list_one_big)] += list_one_big\n",
    "        list_one_big = c\n",
    "    return (np.sqrt(np.sum((list_one_big - list_two_big)**2)))\n",
    "\n",
    "# calculate the Distance for KNN\n",
    "def calculateDist(currentBucket, train_data, test_point):\n",
    "    train_bucket = filter(lambda x: (x != -1), currentBucket)\n",
    "    dist = map(lambda point: cosineLength( test_point.features.indices, train_data[int(point)].features.indices), currentBucket)\n",
    "    return dist\n",
    "\n",
    "\n",
    "# Calculate KNN \n",
    "def calcKnn(train_bucket, test_bucket,train_data,test_data):\n",
    "    classes = np.zeros(2)\n",
    "    predict = np.zeros(test_bucket.size)\n",
    "    for index, test_point in enumerate(test_bucket):\n",
    "        currentBucket = train_bucket[test_point]\n",
    "        classes = np.zeros(2)\n",
    "        final_dist = calculateDist(currentBucket, train_data, test_data[index]) # Calculate the final distance\n",
    "        dist = np.argsort(final_dist, kind='mergesort') # Sort distance\n",
    "        for i in range(0,5):\n",
    "            count = 0\n",
    "            size = 0\n",
    "            if point == -1:\n",
    "                continue\n",
    "            elif train_data[dist[i]].label == 0.0:\n",
    "                classes[0] += 1\n",
    "            else:\n",
    "                classes[1] += 1\n",
    "        # Determine Prediction\n",
    "        if classes[0] >= classes[1]:\n",
    "            predict[index] = 0\n",
    "        else:\n",
    "            predict[index] = 1\n",
    "        print (\"Prediction: \", int(predict[index]), \"Actual: \", int(test_data[index].label))\n",
    "        if int(predict[index]) == int(test_data[index].label):\n",
    "            count += 1 \n",
    "        size += 1\n",
    "#      predict\n",
    "    return predict\n",
    "                \n",
    "predict_label = calcKnn(bucket_train, test_bucket, train_data, test_data)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(0, predict_label.size):\n",
    "    if int(test_data[i].label) == int(predict_label[i]):\n",
    "        count += 1\n",
    "print count\n",
    "print predict_label.size\n",
    "print (str(count*100/predict_label.size)+\"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import hashlib\n",
    "\n",
    "# def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "#     mapping = {}\n",
    "#     print rawFeats\n",
    "#     for ind, category in rawFeats:\n",
    "#         featureString = category + str(ind)\n",
    "#         mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "#     if(printMapping): print mapping\n",
    "#     sparseFeatures = defaultdict(float)\n",
    "#     for bucket in mapping.values():\n",
    "#         sparseFeatures[bucket] += 1.0\n",
    "#     return dict(sparseFeatures)\n",
    "\n",
    "# def paresHashPoint(point, numBuckets):\n",
    "#     fields = point.split(',')\n",
    "#     label = fields[0]\n",
    "#     features = parsePoint(point)\n",
    "#     return LabeledPoint(label, SparseVector(numBuckets, hashFunction(numBuckets, features)))\n",
    "\n",
    "# numBucketsCTR = 2 ** 15\n",
    "# hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point, numBucketsCTR))\n",
    "# hashTrainData.cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
